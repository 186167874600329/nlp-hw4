import nltk
nltk.data.path.append('/scratch/yy5074/nltk_data/tokenizers/punkt/PY3')
nltk.data.path.append('/scratch/yy5074/nltk_data')



import random
from nltk.corpus import wordnet
from nltk import word_tokenize
from nltk.tokenize.treebank import TreebankWordDetokenizer



# è¿™ä¸ªæ˜¯æ²¡ç”¨çš„æ¨¡æ¿å‡½æ•°ï¼ˆä¸åˆ ä¹Ÿæ²¡å…³ç³»ï¼‰
def example_transform(example):
    example["text"] = example["text"].lower()
    return example


def custom_transform(example):
    text = example["text"]
    words = word_tokenize(text)
    detok = TreebankWordDetokenizer()

    def synonym_or_typo(word):
        # è·³è¿‡æ ‡ç‚¹æˆ–æ•°å­—
        if not word.isalpha():
            return word

        # ğŸ”¹10% å‡ ç‡è¿›è¡ŒåŒä¹‰è¯æ›¿æ¢
        if random.random() < 0.25:
            synsets = wordnet.synsets(word)
            if synsets:
                lemmas = [l.name().replace("_", " ") for l in synsets[0].lemmas()]
                # åªå–ä¸åŒçš„è¯
                for lemma in lemmas:
                    if lemma.lower() != word.lower():
                        return lemma

        # ğŸ”¹10% å‡ ç‡åˆ¶é€ æ‹¼å†™é”™è¯¯ï¼ˆäº¤æ¢ç›¸é‚»å­—æ¯ï¼‰
        if random.random() < 0.25 and len(word) > 3:
            i = random.randint(0, len(word) - 2)
            w_list = list(word)
            w_list[i], w_list[i + 1] = w_list[i + 1], w_list[i]
            return "".join(w_list)

        # å¦åˆ™è¿”å›åŸå•è¯
        return word

    # å¯¹å¥å­ä¸­æ¯ä¸ªè¯åšæ›¿æ¢/æ‰°åŠ¨
    new_words = [synonym_or_typo(w) for w in words]
    example["text"] = detok.detokenize(new_words)
    return example
